import os
from typing import List

import numpy as np
import torch
from fastapi import FastAPI, HTTPException
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline
from pydantic import BaseModel

from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer

# ä¼˜å…ˆä½¿ç”¨GPUï¼Œå®Œå…¨é¿å…CPUè®¡ç®—
print("ğŸ”§ æ­£åœ¨é…ç½®GPUä¼˜åŒ–è®¾ç½®...")

# è®¾ç½®ç¯å¢ƒå˜é‡å¼ºåˆ¶ä½¿ç”¨GPU
# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè®©Dockerç¯å¢ƒå˜é‡ç”Ÿæ•ˆ
# os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # æ³¨é‡Šæ‰ç¡¬ç¼–ç ï¼Œè®©Dockerç¯å¢ƒå˜é‡ç”Ÿæ•ˆ
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # é¿å…tokenizerçš„CPUå¹¶è¡Œ

# æåº¦é™åˆ¶CPUçº¿ç¨‹ä½¿ç”¨ï¼Œå¼ºåˆ¶ä½¿ç”¨GPU
torch.set_num_threads(1)
torch.set_num_interop_threads(1)

# æ£€æŸ¥GPUå¯ç”¨æ€§å¹¶æ‰“å°è¯¦ç»†ä¿¡æ¯
if torch.cuda.is_available():
    gpu_count = torch.cuda.device_count()
    current_gpu = torch.cuda.current_device()
    gpu_name = torch.cuda.get_device_name(current_gpu)
    gpu_memory = torch.cuda.get_device_properties(current_gpu).total_memory / (1024**3)
    
    print("ğŸ® GPUè®¾å¤‡ä¿¡æ¯:")
    print(f"   - å¯ç”¨GPUæ•°é‡: {gpu_count}")
    print(f"   - å½“å‰ä½¿ç”¨GPU: {current_gpu}")
    print(f"   - GPUåç§°: {gpu_name}")
    print(f"   - GPUæ€»æ˜¾å­˜: {gpu_memory:.2f} GB")
    
    # å¯ç”¨æ‰€æœ‰GPUä¼˜åŒ–é€‰é¡¹
    torch.backends.cudnn.enabled = True
    torch.backends.cudnn.benchmark = True  # ä¼˜åŒ–å·ç§¯æ“ä½œ
    torch.backends.cuda.matmul.allow_tf32 = True  # å…è®¸TF32
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True
    
    # è®¾ç½®GPUå†…å­˜ç­–ç•¥
    torch.cuda.empty_cache()  # æ¸…ç©ºç¼“å­˜
    print("âœ… GPUä¼˜åŒ–è®¾ç½®å®Œæˆ")
else:
    print("âŒ è­¦å‘Š: æœªæ£€æµ‹åˆ°CUDA GPUï¼Œå°†ä½¿ç”¨CPUï¼ˆæ€§èƒ½ä¼šå¾ˆå·®ï¼‰")

app = FastAPI(title="BGE Model Service")

# æ·»åŠ CORSä¸­é—´ä»¶ä»¥å…è®¸è·¨åŸŸè®¿é—®
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å…è®¸æ‰€æœ‰æ¥æºï¼ˆç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥é™åˆ¶å…·ä½“åŸŸåï¼‰
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ¨¡å‹è·¯å¾„
EMBEDDING_MODEL_PATH = os.getenv("EMBEDDING_MODEL_PATH", "/app/models/bge-large-zh-v1.5")
RERANKER_MODEL_PATH = os.getenv("RERANKER_MODEL_PATH", "/app/models/bge-reranker-large")
CHAT_MODEL_PATH = os.getenv("CHAT_MODEL_PATH", "/app/models/gpt-oss-20b")

# è®¾å¤‡å¸¸é‡
CUDA_DEVICE = "cuda:0"
DEFAULT_DEVICE = CUDA_DEVICE if torch.cuda.is_available() else "cpu"

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹å’Œtokenizer
embedding_tokenizer = None
embedding_model = None
reranker_tokenizer = None
reranker_model = None
chat_tokenizer = None
chat_model = None

# LangChain ç›¸å…³å˜é‡
langchain_llm = None
langchain_chat_model = None
chat_prompt_template = None

# -------------------- ç›‘æ§ä¸ç»Ÿè®¡è¾…åŠ©å‡½æ•° --------------------
def _print_gpu_status():
    """æ‰“å°è¯¦ç»†çš„GPUä½¿ç”¨çŠ¶æ€"""
    if not torch.cuda.is_available():
        print("âŒ GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        return
    
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        allocated = torch.cuda.memory_allocated(i) / (1024**3)
        reserved = torch.cuda.memory_reserved(i) / (1024**3)
        total = props.total_memory / (1024**3)
        
        print(f"ğŸ“Š GPU {i} ({props.name}):")
        print(f"   - å·²åˆ†é…æ˜¾å­˜: {allocated:.2f} GB")
        print(f"   - å·²ä¿ç•™æ˜¾å­˜: {reserved:.2f} GB") 
        print(f"   - æ€»æ˜¾å­˜: {total:.2f} GB")
        print(f"   - æ˜¾å­˜ä½¿ç”¨ç‡: {(allocated/total)*100:.1f}%")

def _check_gpu_utilization():
    """æ£€æŸ¥GPUåˆ©ç”¨ç‡ï¼ˆéœ€è¦nvidia-ml-pyåº“ï¼‰"""
    try:
        import pynvml
        pynvml.nvmlInit()
        handle = pynvml.nvmlDeviceGetHandleByIndex(0)
        utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
        return utilization.gpu
    except Exception:
        return None

def _gpu_memory_snapshot():
    if not torch.cuda.is_available():
        return []
    snapshots = []
    num = torch.cuda.device_count()
    for idx in range(num):
        try:
            torch.cuda.synchronize(idx)
        except Exception:
            pass
        stats = {}
        stats["device_index"] = idx
        stats["name"] = torch.cuda.get_device_name(idx)
        try:
            free_bytes, total_bytes = torch.cuda.mem_get_info(idx)
            stats["total_GB"] = round(total_bytes / 1024**3, 2)
            stats["free_GB"] = round(free_bytes / 1024**3, 2)
        except Exception:
            stats["total_GB"] = stats["free_GB"] = None
        stats["allocated_GB"] = round(torch.cuda.memory_allocated(idx) / 1024**3, 2)
        stats["reserved_GB"] = round(torch.cuda.memory_reserved(idx) / 1024**3, 2)
        
        # å°è¯•è·å–GPUåˆ©ç”¨ç‡
        gpu_util = _check_gpu_utilization() if idx == 0 else None
        if gpu_util is not None:
            stats["gpu_utilization"] = gpu_util
            
        snapshots.append(stats)
    return snapshots

def _model_param_stats():
    def _count(m):
        if m is None:
            return None
        try:
            return sum(p.numel() for p in m.parameters())
        except Exception:
            return None
    return {
        "embedding_params": _count(embedding_model),
        "reranker_params": _count(reranker_model),
        "chat_params": _count(chat_model),
    }

def _process_memory():
    rss = None
    try:
        # å°è¯• psutil
        import psutil  # type: ignore
        p = psutil.Process(os.getpid())
        rss = p.memory_info().rss
    except Exception:
        # Linux /proc/self/statm å›é€€
        try:
            with open("/proc/self/statm", "r") as f:
                parts = f.read().strip().split()
                if len(parts) >= 2:
                    page_size = os.sysconf("SC_PAGE_SIZE")
                    rss = int(parts[1]) * page_size
        except Exception:
            rss = None
    if rss is None:
        return None
    return {
        "rss_GB": round(rss / 1024**3, 3)
    }

class TextRequest(BaseModel):
    texts: List[str]
    max_length: int = 512

class EmbeddingResponse(BaseModel):
    embeddings: List[List[float]]

class RerankRequest(BaseModel):
    query: str
    documents: List[str]
    max_length: int = 512

class RerankResponse(BaseModel):
    scores: List[float]

class ChatRequest(BaseModel):
    messages: List[dict]  # [{"role": "user", "content": "..."}]
    max_length: int = 2048
    temperature: float = 0.7
    do_sample: bool = True

class ChatResponse(BaseModel):
    message: str
    model: str

def _load_embedding():
    global embedding_tokenizer, embedding_model
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½Embeddingæ¨¡å‹: {EMBEDDING_MODEL_PATH}")
    
    embedding_tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_PATH)
    embedding_model = AutoModel.from_pretrained(EMBEDDING_MODEL_PATH)
    
    # ç§»åŠ¨åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if torch.cuda.is_available():
        embedding_model = embedding_model.to(DEFAULT_DEVICE)
        print(f"âœ… Embeddingæ¨¡å‹å·²åŠ è½½åˆ°: {next(embedding_model.parameters()).device}")
        print(f"ğŸ“Š æ¨¡å‹æ•°æ®ç±»å‹: {next(embedding_model.parameters()).dtype}")
    else:
        print("âš ï¸  è­¦å‘Š: Embeddingæ¨¡å‹åŠ è½½åˆ°CPU")
    
    embedding_model.eval()
    _print_gpu_status()

def _load_reranker():
    global reranker_tokenizer, reranker_model
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½Rerankeræ¨¡å‹: {RERANKER_MODEL_PATH}")
    
    reranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL_PATH)
    reranker_model = AutoModel.from_pretrained(RERANKER_MODEL_PATH)
    
    # ç§»åŠ¨åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if torch.cuda.is_available():
        reranker_model = reranker_model.to(DEFAULT_DEVICE)
        print(f"âœ… Rerankeræ¨¡å‹å·²åŠ è½½åˆ°: {next(reranker_model.parameters()).device}")
        print(f"ğŸ“Š æ¨¡å‹æ•°æ®ç±»å‹: {next(reranker_model.parameters()).dtype}")
    else:
        print("âš ï¸  è­¦å‘Š: Rerankeræ¨¡å‹åŠ è½½åˆ°CPU")
    
    reranker_model.eval()
    _print_gpu_status()

def _load_chat():
    global chat_tokenizer, chat_model, langchain_llm, langchain_chat_model, chat_prompt_template
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½Chatæ¨¡å‹: {CHAT_MODEL_PATH}")
    print("ğŸ”§ ä½¿ç”¨æ¨¡å‹åŸç”Ÿé…ç½®")
    
    chat_tokenizer = AutoTokenizer.from_pretrained(CHAT_MODEL_PATH)
    
    # å¼ºåˆ¶GPUè®¾å¤‡æ˜ å°„ç­–ç•¥
    if torch.cuda.is_available():
        print("ğŸ® é…ç½®GPUè®¾å¤‡æ˜ å°„...")
        
        # æ£€æŸ¥GPUæ˜¾å­˜
        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"ğŸ’¾ å¯ç”¨GPUæ˜¾å­˜: {gpu_memory_gb:.2f} GB")
        
        # æ ¹æ®æ˜¾å­˜å¤§å°é€‰æ‹©ç­–ç•¥
        if gpu_memory_gb >= 24:  # 24GBä»¥ä¸Šæ˜¾å­˜
            device_map = {"": 0}  # å…¨éƒ¨æ”¾åœ¨GPU 0
            print("ğŸ“ ç­–ç•¥: æ‰€æœ‰å±‚æ”¾åœ¨GPU 0")
        elif gpu_memory_gb >= 12:  # 12-24GBæ˜¾å­˜
            device_map = "auto"  # è‡ªåŠ¨åˆ†é…ä½†ä¼˜å…ˆGPU
            print("ğŸ“ ç­–ç•¥: è‡ªåŠ¨åˆ†é…ï¼Œä¼˜å…ˆGPU")
        else:  # æ˜¾å­˜ä¸è¶³
            print("âš ï¸  è­¦å‘Š: GPUæ˜¾å­˜ä¸è¶³12GBï¼Œå¯èƒ½å½±å“æ€§èƒ½")
            device_map = "auto"
    else:
        device_map = "cpu"
        print("âŒ ä½¿ç”¨CPUè®¾å¤‡æ˜ å°„")
    
    # åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨æœ€ç®€é…ç½®
    print("ğŸ”„ å¼€å§‹è°ƒç”¨ AutoModelForCausalLM.from_pretrained...")
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {CHAT_MODEL_PATH}")
    
    import time
    start_time = time.time()
    
    try:
        # ä½¿ç”¨accelerateçš„è‡ªåŠ¨è®¾å¤‡æ˜ å°„
        print("â³ æ­£åœ¨å®ä¾‹åŒ–æ¨¡å‹...")
        chat_model = AutoModelForCausalLM.from_pretrained(
            CHAT_MODEL_PATH,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            torch_dtype=torch.bfloat16,  # æ˜ç¡®æŒ‡å®šæ•°æ®ç±»å‹
            device_map="auto"  # è®©accelerateè‡ªåŠ¨ç®¡ç†è®¾å¤‡æ˜ å°„
        )
        
        end_time = time.time()
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’")
        
        # ä¸éœ€è¦æ‰‹åŠ¨ç§»åŠ¨åˆ°GPUï¼Œaccelerateå·²ç»å¤„ç†äº†
        print("âœ… æ¨¡å‹å·²é€šè¿‡accelerateè‡ªåŠ¨æ˜ å°„åˆ°GPU")
        
        # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨
        _print_gpu_status()
        
        print("ğŸ”„ å¼€å§‹æ£€æŸ¥æ¨¡å‹çŠ¶æ€...")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        raise e
    
    
    print("ğŸ”„ æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¼€å§‹åå¤„ç†...")
    chat_model.eval()
    print("âœ… æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼")
    
    # å…ˆæ‰“å°åŸºæœ¬çš„æ¨¡å‹åŠ è½½çŠ¶æ€
    print("âœ… Chatæ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡")
    print("ğŸ“Š Chatæ¨¡å‹è®¾å¤‡åˆ†å¸ƒ:")
    try:
        if hasattr(chat_model, 'hf_device_map'):
            device_map_items = list(chat_model.hf_device_map.items())[:5]  # åªæ˜¾ç¤ºå‰5ä¸ªé¿å…è¾“å‡ºè¿‡å¤š
            for layer, device in device_map_items:
                if isinstance(layer, str) and len(layer) > 50:
                    layer = layer[:47] + "..."
                print(f"   {layer}: {device}")
            if len(chat_model.hf_device_map) > 5:
                print(f"   ... å’Œå…¶ä»– {len(chat_model.hf_device_map) - 5} ä¸ªå±‚")
        else:
            print("   æœªæ‰¾åˆ°è®¾å¤‡æ˜ å°„ä¿¡æ¯")
    except Exception as e:
        print(f"   è®¾å¤‡æ˜ å°„æ£€æŸ¥å¤±è´¥: {e}")
    
    print("ğŸ” å¼€å§‹æ£€æŸ¥å‚æ•°åˆ†å¸ƒ...")
    # éªŒè¯æ¨¡å‹æ˜¯å¦åœ¨GPUä¸Š
    try:
        gpu_params = 0
        cpu_params = 0
        total_checked = 0
        
        for name, param in chat_model.named_parameters():
            if param.device.type == 'cuda':
                gpu_params += 1
            else:
                cpu_params += 1
            total_checked += 1
            
            # æ¯æ£€æŸ¥1000ä¸ªå‚æ•°æ‰“å°ä¸€æ¬¡è¿›åº¦ï¼Œé¿å…å¡ä½
            if total_checked % 1000 == 0:
                print(f"   å·²æ£€æŸ¥å‚æ•°: {total_checked}")
        
        total_params = gpu_params + cpu_params
        gpu_percentage = (gpu_params / total_params * 100) if total_params > 0 else 0
        
        print("ğŸ¯ æ¨¡å‹å‚æ•°åˆ†å¸ƒ:")
        print(f"   - GPUå‚æ•°: {gpu_params}/{total_params} ({gpu_percentage:.1f}%)")
        print(f"   - CPUå‚æ•°: {cpu_params}/{total_params} ({100-gpu_percentage:.1f}%)")
        
        if gpu_percentage < 80:
            print("âš ï¸  è­¦å‘Š: è¶…è¿‡20%çš„å‚æ•°åœ¨CPUä¸Šï¼Œè¿™ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼")
        else:
            print("âœ… æ¨¡å‹ä¸»è¦åœ¨GPUä¸Šï¼Œæ€§èƒ½è‰¯å¥½")
            
    except Exception as e:
        print(f"âŒ å‚æ•°åˆ†å¸ƒæ£€æŸ¥å¤±è´¥: {e}")
    
    print("âœ… Chatæ¨¡å‹åŸºç¡€åŠ è½½å®Œæˆ (MXFP4)")
    _print_gpu_status()
    
    # é›†æˆ LangChain
    print("ğŸ”— æ­£åœ¨é›†æˆ LangChain...")
    try:
        # ä½¿ç”¨æ­£ç¡®çš„ HuggingFacePipeline åˆå§‹åŒ–æ–¹å¼
        from transformers import pipeline

        # åˆ›å»º transformers pipeline
        hf_pipeline = pipeline(
            "text-generation",
            model=chat_model,
            tokenizer=chat_tokenizer,
            # ä¸æŒ‡å®šdeviceå‚æ•°ï¼Œå› ä¸ºæ¨¡å‹å·²é€šè¿‡accelerateç®¡ç†è®¾å¤‡
            model_kwargs={
                "temperature": 0.3,
                "max_new_tokens": 512,
                "do_sample": True,
                "pad_token_id": chat_tokenizer.eos_token_id,
                "eos_token_id": chat_tokenizer.eos_token_id,
            }
        )
        
        # åˆ›å»º LangChain Pipeline
        langchain_llm = HuggingFacePipeline(pipeline=hf_pipeline)
        
        # åˆ›å»º Chat æ¨¡å‹
        langchain_chat_model = ChatHuggingFace(llm=langchain_llm, verbose=False)
        
        # åˆ›å»ºå¯¹è¯æ¨¡æ¿
        chat_prompt_template = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚è¯·ç®€æ´ã€å‡†ç¡®åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸è¦åŒ…å«ä»»ä½•å¤šä½™çš„å†…å®¹æˆ–è§£é‡Šã€‚"),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}")
        ])
        
        print("âœ… LangChain é›†æˆå®Œæˆ")
        
    except Exception as e:
        print(f"âš ï¸ LangChain é›†æˆå¤±è´¥ï¼Œå°†ä½¿ç”¨åŸç”Ÿæ¨¡å¼: {e}")
        import traceback
        print("ğŸ› è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()
        langchain_llm = None
        langchain_chat_model = None
        chat_prompt_template = None

def _maybe_warmup():
    if os.getenv("CHAT_WARMUP", "0") != "1":
        return
    if chat_model is None:
        return
    try:
        print("Warmup start ...")
        t = chat_tokenizer("Hello", return_tensors="pt")
        # æ‰¾åˆ° chat_model çš„ä¸€ä¸ªå·²åŠ è½½è®¾å¤‡
        try:
            device = next(p.device for p in chat_model.parameters() if p.device.type != "meta")
            t = {k: v.to(device) for k, v in t.items()}
        except StopIteration:
            # å¦‚æœæ‰€æœ‰å‚æ•°éƒ½åœ¨ meta è®¾å¤‡ï¼Œä½¿ç”¨é»˜è®¤è®¾å¤‡
            t = {k: v.to(DEFAULT_DEVICE) for k, v in t.items()}
        with torch.no_grad():
            chat_model.generate(**t, max_new_tokens=4)
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        print("Warmup done")
    except Exception as e:
        print(f"Warmup failed: {e}")

@app.on_event("startup")
async def load_models():
    global embedding_tokenizer, embedding_model, reranker_tokenizer, reranker_model, chat_tokenizer, chat_model
    
    print("ğŸš€ å¯åŠ¨BGEæ¨¡å‹æœåŠ¡...")
    print("=" * 60)
    
    # æ‰“å°åˆå§‹GPUçŠ¶æ€
    print("ğŸ” æ£€æŸ¥GPUç¯å¢ƒ:")
    _print_gpu_status()
    
    os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"
    
    app.state.chat_config = {
        "quant": "mxfp4",
        "resolved_dtype": "native",
        "max_new_tokens_default": int(os.getenv("MAX_NEW_TOKENS", "512"))
    }
    
    print("ğŸ¯ é…ç½®ä¿¡æ¯:")
    print(f"   - é‡åŒ–æ–¹å¼: {app.state.chat_config['quant']}")
    print(f"   - æ•°æ®ç±»å‹: {app.state.chat_config['resolved_dtype']}")
    print(f"   - é»˜è®¤æœ€å¤§Token: {app.state.chat_config['max_new_tokens_default']}")
    
    try:
        print("\nğŸ“¦ å¼€å§‹åŠ è½½æ¨¡å‹...")
        _load_embedding()
        print()
        _load_reranker() 
        print()
        _load_chat()
        print()
        _maybe_warmup()
        
        print("\nâœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼")
        print("ğŸ® æœ€ç»ˆGPUçŠ¶æ€:")
        _print_gpu_status()
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½é”™è¯¯: {e}")
        import traceback
        print("ğŸ› è¯¦ç»†é”™è¯¯è¿½è¸ª:")
        traceback.print_exc()
        print("âš ï¸  æœåŠ¡å°†ä»¥å¯ç”¨ç»„ä»¶ç»§ç»­è¿è¡Œ")
        _print_gpu_status()

@app.get("/")
async def root():
    return {"message": "BGE Model Service is running"}

@app.get("/health")
async def health_check():
    # æ”¶é›†GPUä¿¡æ¯
    gpu_info = {}
    if torch.cuda.is_available():
        gpu_info = {
            "gpu_available": True,
            "gpu_count": torch.cuda.device_count(),
            "current_device": torch.cuda.current_device(),
            "gpu_name": torch.cuda.get_device_name(0),
            "gpu_memory_allocated_gb": round(torch.cuda.memory_allocated(0) / (1024**3), 2),
            "gpu_memory_reserved_gb": round(torch.cuda.memory_reserved(0) / (1024**3), 2),
            "gpu_memory_total_gb": round(torch.cuda.get_device_properties(0).total_memory / (1024**3), 2)
        }
    else:
        gpu_info = {"gpu_available": False}
    
    # æ£€æŸ¥æ¨¡å‹è®¾å¤‡ä½ç½®
    model_devices = {}
    if embedding_model is not None:
        model_devices["embedding_device"] = str(next(embedding_model.parameters()).device)
    if reranker_model is not None:
        model_devices["reranker_device"] = str(next(reranker_model.parameters()).device)
    if chat_model is not None:
        # æ£€æŸ¥chatæ¨¡å‹çš„è®¾å¤‡åˆ†å¸ƒ
        gpu_layers = 0
        cpu_layers = 0
        for param in chat_model.parameters():
            if param.device.type == 'cuda':
                gpu_layers += 1
            else:
                cpu_layers += 1
        
        model_devices["chat_gpu_params"] = gpu_layers
        model_devices["chat_cpu_params"] = cpu_layers
        model_devices["chat_gpu_percentage"] = round(gpu_layers / (gpu_layers + cpu_layers) * 100, 1) if (gpu_layers + cpu_layers) > 0 else 0
    
    return {
        "status": "healthy", 
        "embedding_model_loaded": embedding_model is not None,
        "reranker_model_loaded": reranker_model is not None,
        "chat_model_loaded": chat_model is not None,
        "chat_quant": "mxfp4",
        "chat_dtype": getattr(app.state, 'chat_config', {}).get('resolved_dtype'),
        "max_new_tokens_default": getattr(app.state, 'chat_config', {}).get('max_new_tokens_default'),
        "gpu_info": gpu_info,
        "model_devices": model_devices
    }

@app.get("/stats")
async def stats():
    return {
        "gpu_memory": _gpu_memory_snapshot(),
        "models": _model_param_stats(),
        "process_memory": _process_memory(),
        "chat_config": getattr(app.state, 'chat_config', {}),
    }

@app.post("/embeddings", response_model=EmbeddingResponse)
async def get_embeddings(request: TextRequest):
    if embedding_model is None or embedding_tokenizer is None:
        raise HTTPException(status_code=500, detail="Embedding model not loaded")
    
    try:
        # ç¼–ç æ–‡æœ¬
        encoded_input = embedding_tokenizer(
            request.texts,
            padding=True,
            truncation=True,
            max_length=request.max_length,
            return_tensors='pt'
        )
        
        # å°†è¾“å…¥ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
        device = next(embedding_model.parameters()).device
        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
        
        # è·å–åµŒå…¥å‘é‡
        with torch.no_grad():
            model_output = embedding_model(**encoded_input)
            embeddings = model_output.last_hidden_state[:, 0].cpu().numpy()
        
        return EmbeddingResponse(embeddings=embeddings.tolist())
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error processing request: {str(e)}")

@app.post("/rerank", response_model=RerankResponse)
async def rerank_documents(request: RerankRequest):
    if reranker_model is None or reranker_tokenizer is None:
        raise HTTPException(status_code=500, detail="Reranker model not loaded")
    
    try:
        device = next(reranker_model.parameters()).device
        scores = []
        for doc in request.documents:
            # ä¸ºé‡æ’åºæ¨¡å‹å‡†å¤‡è¾“å…¥å¯¹
            pairs = [[request.query, doc]]
            
            # ç¼–ç æŸ¥è¯¢-æ–‡æ¡£å¯¹
            encoded_input = reranker_tokenizer(
                pairs,
                padding=True,
                truncation=True,
                max_length=request.max_length,
                return_tensors='pt'
            )
            
            # å°†è¾“å…¥ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
            encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
            
            # è·å–é‡æ’åºåˆ†æ•°
            with torch.no_grad():
                outputs = reranker_model(**encoded_input)
                # è·å–CLS tokençš„è¾“å‡ºä½œä¸ºç›¸å…³æ€§åˆ†æ•°
                score = outputs.last_hidden_state[:, 0].cpu().numpy()[0]
                # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°ï¼ˆè¿™é‡Œä½¿ç”¨ç®€å•çš„å‡å€¼ï¼Œå®é™…å¯èƒ½éœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹è°ƒæ•´ï¼‰
                relevance_score = float(np.mean(score))
                scores.append(relevance_score)
        
        return RerankResponse(scores=scores)
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error processing rerank request: {str(e)}")

def _get_model_device(model):
    """è·å–æ¨¡å‹çš„è®¾å¤‡"""
    device = None
    if torch.cuda.is_available():
        try:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰device_map
            if hasattr(model, 'hf_device_map'):
                # è·å–ç¬¬ä¸€ä¸ªå¯ç”¨è®¾å¤‡
                device = next(iter(model.hf_device_map.values()))
                if isinstance(device, str):
                    device = torch.device(device)
                elif isinstance(device, int):
                    device = torch.device(f"cuda:{device}")
            else:
                # å°è¯•ä»æ¨¡å‹å‚æ•°è·å–è®¾å¤‡
                device = next(p.device for p in model.parameters() if p.device.type != "meta")
        except (StopIteration, AttributeError):
            device = torch.device(DEFAULT_DEVICE)
    else:
        device = torch.device("cpu")
    return device

def _generate_with_model(model, tokenizer, input_ids, attention_mask, max_new_tokens, temperature, do_sample, device):
    """ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬"""
    with torch.no_grad():
        if torch.cuda.is_available() and device.type == "cuda":
            with torch.cuda.amp.autocast():
                outputs = model.generate(
                    input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=do_sample,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    use_cache=True,
                    num_beams=1,
                    early_stopping=True
                )
        else:
            outputs = model.generate(
                input_ids,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=do_sample,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                use_cache=True,
                num_beams=1,
                early_stopping=True
            )
    return outputs

def _chat_with_langchain(messages: List[dict]):
    """ä½¿ç”¨ LangChain è¿›è¡ŒèŠå¤©æ¨ç†"""
    if langchain_chat_model is None or chat_prompt_template is None:
        return None
    
    try:
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        chat_history = []
        current_input = ""
        
        for msg in messages:
            if msg["role"] == "user":
                current_input = msg["content"]  # æœ€åä¸€ä¸ªç”¨æˆ·è¾“å…¥
            elif msg["role"] == "assistant":
                if current_input:  # å¦‚æœæœ‰ä¹‹å‰çš„ç”¨æˆ·è¾“å…¥
                    chat_history.append(HumanMessage(content=current_input))
                    current_input = ""
                chat_history.append(AIMessage(content=msg["content"]))
        
        # æ„å»ºprompt
        formatted_prompt = chat_prompt_template.format_messages(
            chat_history=chat_history,
            input=current_input
        )
        
        # ä½¿ç”¨ LangChain ç”Ÿæˆå›å¤
        response = langchain_chat_model.invoke(formatted_prompt)
        
        # æå–å›å¤å†…å®¹
        if hasattr(response, 'content'):
            return response.content.strip()
        else:
            return str(response).strip()
            
    except Exception as e:
        print(f"âŒ LangChain æ¨ç†å¤±è´¥: {e}")
        return None

@app.post("/chat", response_model=ChatResponse)
async def chat_completion(request: ChatRequest):
    if chat_model is None or chat_tokenizer is None:
        raise HTTPException(status_code=500, detail="Chat model not loaded")
    
    # æ‰“å°æ¨ç†å¼€å§‹çŠ¶æ€
    print(f"ğŸš€ å¼€å§‹Chatæ¨ç†ï¼Œè¾“å…¥é•¿åº¦: {len(request.messages)} æ¡æ¶ˆæ¯")
    _print_gpu_status()
    
    try:
        # ä¼˜å…ˆå°è¯•ä½¿ç”¨ LangChain
        if langchain_chat_model is not None:
            print("ğŸ”— ä½¿ç”¨ LangChain æ¨¡å¼æ¨ç†")
            assistant_response = _chat_with_langchain(request.messages)
            
            if assistant_response:
                print("âœ… LangChain æ¨ç†å®Œæˆ")
                _print_gpu_status()
                return ChatResponse(
                    message=assistant_response,
                    model=CHAT_MODEL_PATH.split("/")[-1] + " (LangChain)"
                )
            else:
                print("âš ï¸ LangChain æ¨ç†å¤±è´¥ï¼Œå›é€€åˆ°åŸç”Ÿæ¨¡å¼")
        
        # å›é€€åˆ°åŸç”Ÿæ¨¡å¼
        print("ğŸ”§ ä½¿ç”¨åŸç”Ÿæ¨¡å¼æ¨ç†")
        # æ„å»ºå¯¹è¯prompt
        conversation = ""
        for message in request.messages:
            role = message["role"]
            content = message["content"]
            if role == "user":
                conversation += f"User: {content}\n"
            elif role == "assistant":
                conversation += f"Assistant: {content}\n"
        
        conversation += "Assistant: "
        print(f"ğŸ“ å¯¹è¯ä¸Šä¸‹æ–‡é•¿åº¦: {len(conversation)} å­—ç¬¦")
        
        # ç¼–ç è¾“å…¥ - å¼ºåˆ¶ä½¿ç”¨GPUå‹å¥½çš„è®¾ç½®
        encoded = chat_tokenizer(
            conversation,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        )
        
        print(f"ğŸ”¢ Tokenæ•°é‡: {encoded['input_ids'].shape[1]}")
        
        # è·å–è®¾å¤‡å¹¶ç§»åŠ¨æ•°æ®åˆ°GPU
        device = _get_model_device(chat_model)
        print(f"ğŸ“± æ¨ç†è®¾å¤‡: {device}")
        
        # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        input_ids = encoded["input_ids"].to(device, non_blocking=True)
        attention_mask = encoded["attention_mask"].to(device, non_blocking=True)
        
        print("âœ… è¾“å…¥æ•°æ®å·²ç§»åŠ¨åˆ°GPU")
        
        # ç”Ÿæˆå›å¤ - ä½¿ç”¨ä¼˜åŒ–çš„ç”Ÿæˆå‡½æ•°
        max_new_tokens = min(request.max_length, getattr(app.state, 'chat_config', {}).get('max_new_tokens_default', request.max_length))
        print(f"ğŸ¯ æœ€å¤§ç”ŸæˆTokenæ•°: {max_new_tokens}")
        
        # å¼€å§‹æ¨ç†å¹¶ç›‘æ§
        print("âš¡ å¼€å§‹GPUæ¨ç†...")
        start_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None
        end_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None
        
        if start_time:
            start_time.record()
            
        outputs = _generate_with_model(
            chat_model, chat_tokenizer, input_ids, attention_mask,
            max_new_tokens, request.temperature, request.do_sample, device
        )
        
        if end_time:
            end_time.record()
            torch.cuda.synchronize()
            inference_time = start_time.elapsed_time(end_time) / 1000.0  # è½¬æ¢ä¸ºç§’
            print(f"â±ï¸  GPUæ¨ç†æ—¶é—´: {inference_time:.3f} ç§’")
        
        # è§£ç è¾“å‡º
        response = chat_tokenizer.decode(outputs[0], skip_special_tokens=True)
        assistant_response = response[len(conversation):]
        
        # è®¡ç®—ç”Ÿæˆçš„tokenæ•°
        generated_tokens = outputs[0].shape[0] - input_ids.shape[1]
        print(f"ğŸ“Š ç”ŸæˆTokenæ•°: {generated_tokens}")
        
        if start_time and generated_tokens > 0:
            tokens_per_second = generated_tokens / inference_time
            print(f"ğŸš€ ç”Ÿæˆé€Ÿåº¦: {tokens_per_second:.2f} tokens/ç§’")
        
        print("âœ… Chatæ¨ç†å®Œæˆ")
        _print_gpu_status()
        
        return ChatResponse(
            message=assistant_response.strip(),
            model=CHAT_MODEL_PATH.split("/")[-1]
        )
    
    except Exception as e:
        print(f"âŒ Chatæ¨ç†å¤±è´¥: {str(e)}")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("ğŸ§¹ å·²æ¸…ç†GPUç¼“å­˜")
        raise HTTPException(status_code=500, detail=f"Error processing chat request: {str(e)}")

if __name__ == "__main__":
    print("ğŸŒŸ å¯åŠ¨BGEæ¨¡å‹æœåŠ¡å™¨")
    print("ğŸ® GPUæ£€æŸ¥:")
    _print_gpu_status()
    
    import uvicorn
    uvicorn.run(
        app, 
        host="0.0.0.0",  # ç¡®ä¿ç»‘å®šåˆ°æ‰€æœ‰ç½‘ç»œæ¥å£ï¼Œå…è®¸å¤–éƒ¨è®¿é—®
        port=8000,       # Dockerå®¹å™¨å†…éƒ¨ä½¿ç”¨8000ç«¯å£
        log_level="info",
        access_log=True
    )