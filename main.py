import os
from typing import List

import numpy as np
import torch
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline
from pydantic import BaseModel

from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer

# ä¼˜å…ˆä½¿ç”¨GPUï¼Œå®Œå…¨é¿å…CPUè®¡ç®—
print("ğŸ”§ æ­£åœ¨é…ç½®GPUä¼˜åŒ–è®¾ç½®...")

# è®¾ç½®ç¯å¢ƒå˜é‡å¼ºåˆ¶ä½¿ç”¨GPUå’Œä¼˜åŒ–æ˜¾å­˜
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # ä½¿ç”¨ç¬¬ä¸€å—GPU
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # é¿å…tokenizerçš„CPUå¹¶è¡Œ
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"  # å‡å°‘æ˜¾å­˜ç¢ç‰‡

# æåº¦é™åˆ¶CPUçº¿ç¨‹ä½¿ç”¨ï¼Œå¼ºåˆ¶ä½¿ç”¨GPU
torch.set_num_threads(1)
torch.set_num_interop_threads(1)

# æ£€æŸ¥GPUå¯ç”¨æ€§å¹¶æ‰“å°è¯¦ç»†ä¿¡æ¯
if torch.cuda.is_available():
    gpu_count = torch.cuda.device_count()
    current_gpu = torch.cuda.current_device()
    gpu_name = torch.cuda.get_device_name(current_gpu)
    gpu_memory = torch.cuda.get_device_properties(current_gpu).total_memory / (1024**3)
    
    print("ğŸ® GPUè®¾å¤‡ä¿¡æ¯:")
    print(f"   - å¯ç”¨GPUæ•°é‡: {gpu_count}")
    print(f"   - å½“å‰ä½¿ç”¨GPU: {current_gpu}")
    print(f"   - GPUåç§°: {gpu_name}")
    print(f"   - GPUæ€»æ˜¾å­˜: {gpu_memory:.2f} GB")
    
    # å¯ç”¨æ‰€æœ‰GPUä¼˜åŒ–é€‰é¡¹
    torch.backends.cudnn.enabled = True
    torch.backends.cudnn.benchmark = True  # ä¼˜åŒ–å·ç§¯æ“ä½œ
    torch.backends.cuda.matmul.allow_tf32 = True  # å…è®¸TF32
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True
    
    # è®¾ç½®GPUå†…å­˜ç­–ç•¥
    torch.cuda.empty_cache()  # æ¸…ç©ºç¼“å­˜
    print("âœ… GPUä¼˜åŒ–è®¾ç½®å®Œæˆ")
else:
    print("âŒ è­¦å‘Š: æœªæ£€æµ‹åˆ°CUDA GPUï¼Œå°†ä½¿ç”¨CPUï¼ˆæ€§èƒ½ä¼šå¾ˆå·®ï¼‰")

app = FastAPI(title="BGE Model Service")

# æ¨¡å‹è·¯å¾„
EMBEDDING_MODEL_PATH = os.getenv("EMBEDDING_MODEL_PATH", "/app/models/bge-large-zh-v1.5")
RERANKER_MODEL_PATH = os.getenv("RERANKER_MODEL_PATH", "/app/models/bge-reranker-large")
CHAT_MODEL_PATH = os.getenv("CHAT_MODEL_PATH", "/app/models/qwen3-30b-a3b-instruct-2507")

# è®¾å¤‡å¸¸é‡
CUDA_DEVICE = "cuda:0"
DEFAULT_DEVICE = CUDA_DEVICE if torch.cuda.is_available() else "cpu"

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹å’Œtokenizer
embedding_tokenizer = None
embedding_model = None
reranker_tokenizer = None
reranker_model = None
chat_tokenizer = None
chat_model = None

# LangChain ç›¸å…³å˜é‡
langchain_llm = None
langchain_chat_model = None
chat_prompt_template = None

# æ€§èƒ½ä¼˜åŒ–å¸¸é‡
QWEN_TEMPLATE_CONSTANTS = {
    "IM_END": "<|im_end|>",
    "IM_START_ASSISTANT": "<|im_start|>assistant",
    "IM_START_USER": "<|im_start|>user",
    "IM_START_SYSTEM": "<|im_start|>system"
}

# APIå¸¸é‡
CHAT_COMPLETION_CHUNK = "chat.completion.chunk"

# -------------------- ç›‘æ§ä¸ç»Ÿè®¡è¾…åŠ©å‡½æ•° --------------------
def _get_attention_config():
    """é…ç½® Flash Attention 2"""
    try:
        print("ğŸ” æ­£åœ¨æ£€æµ‹ Flash Attention...")
        import flash_attn
        flash_version = flash_attn.__version__
        print(f"ğŸ“¦ æ£€æµ‹åˆ° Flash Attention ç‰ˆæœ¬: {flash_version}")
        print(f"âœ… ä½¿ç”¨ Flash Attention {flash_version}")
        return {"attn_implementation": "flash_attention_2"}
    except ImportError:
        print("âŒ Flash Attention æœªå®‰è£…ï¼Œä½¿ç”¨é»˜è®¤å®ç°")
        return {}
    except Exception as e:
        print(f"âš ï¸  Flash Attention æ£€æŸ¥å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å®ç°")
        return {}

def _print_gpu_status():
    """æ‰“å°è¯¦ç»†çš„GPUä½¿ç”¨çŠ¶æ€"""
    if not torch.cuda.is_available():
        print("âŒ GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        return
    
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        allocated = torch.cuda.memory_allocated(i) / (1024**3)
        reserved = torch.cuda.memory_reserved(i) / (1024**3)
        total = props.total_memory / (1024**3)
        
        print(f"ğŸ“Š GPU {i} ({props.name}):")
        print(f"   - å·²åˆ†é…æ˜¾å­˜: {allocated:.2f} GB")
        print(f"   - å·²ä¿ç•™æ˜¾å­˜: {reserved:.2f} GB") 
        print(f"   - æ€»æ˜¾å­˜: {total:.2f} GB")
        print(f"   - æ˜¾å­˜ä½¿ç”¨ç‡: {(allocated/total)*100:.1f}%")



class TextRequest(BaseModel):
    texts: List[str]
    max_length: int = 512

class EmbeddingResponse(BaseModel):
    embeddings: List[List[float]]

class RerankRequest(BaseModel):
    query: str
    documents: List[str]
    max_length: int = 512

class RerankResponse(BaseModel):
    scores: List[float]

class ChatRequest(BaseModel):
    messages: List[dict]  # [{"role": "user", "content": "..."}]
    max_length: int = 2048
    temperature: float = 0.7
    do_sample: bool = True

# OpenAI å…¼å®¹çš„è¯·æ±‚å“åº”æ¨¡å‹
class OpenAIMessage(BaseModel):
    role: str  # "system", "user", "assistant"
    content: str

class OpenAIChatRequest(BaseModel):
    model: str = "qwen3-30b-a3b-instruct-2507"
    messages: List[OpenAIMessage]
    max_tokens: int = 2048
    temperature: float = 0.7
    stream: bool = False

class OpenAIChoice(BaseModel):
    message: OpenAIMessage
    finish_reason: str = "stop"
    index: int = 0

class OpenAIUsage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class OpenAIChatResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[OpenAIChoice]
    usage: OpenAIUsage

class ChatResponse(BaseModel):
    message: str
    model: str

def _load_embedding():
    global embedding_tokenizer, embedding_model
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½Embeddingæ¨¡å‹: {EMBEDDING_MODEL_PATH}")
    
    embedding_tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_PATH)
    embedding_model = AutoModel.from_pretrained(EMBEDDING_MODEL_PATH)
    
    # ç§»åŠ¨åˆ°GPU
    embedding_model = embedding_model.to(DEFAULT_DEVICE)
    print(f"âœ… Embeddingæ¨¡å‹å·²åŠ è½½åˆ°: {next(embedding_model.parameters()).device}")
    print(f"ğŸ“Š æ¨¡å‹æ•°æ®ç±»å‹: {next(embedding_model.parameters()).dtype}")
    
    embedding_model.eval()

def _load_reranker():
    global reranker_tokenizer, reranker_model
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½Rerankeræ¨¡å‹: {RERANKER_MODEL_PATH}")
    
    # æŠ‘åˆ¶BGE-rerankerçš„pooleræƒé‡è­¦å‘Š
    import warnings
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", message="Some weights of XLMRobertaModel were not initialized")
        reranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL_PATH)
        reranker_model = AutoModel.from_pretrained(RERANKER_MODEL_PATH)
    
    # ç§»åŠ¨åˆ°GPU
    reranker_model = reranker_model.to(DEFAULT_DEVICE)
    print(f"âœ… Rerankeræ¨¡å‹å·²åŠ è½½åˆ°: {next(reranker_model.parameters()).device}")
    print(f"ğŸ“Š æ¨¡å‹æ•°æ®ç±»å‹: {next(reranker_model.parameters()).dtype}")
    
    reranker_model.eval()

def _load_chat():
    global chat_tokenizer, chat_model, langchain_llm, langchain_chat_model, chat_prompt_template
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½Chatæ¨¡å‹: {CHAT_MODEL_PATH}")
    print("ğŸ”§ ä½¿ç”¨æ¨¡å‹åŸç”Ÿé…ç½®")
    
    chat_tokenizer = AutoTokenizer.from_pretrained(CHAT_MODEL_PATH)
    
    # GPUè®¾å¤‡æ˜ å°„é…ç½®
    print("ğŸ® é…ç½®GPUè®¾å¤‡æ˜ å°„...")
    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    print(f"ğŸ’¾ å¯ç”¨GPUæ˜¾å­˜: {gpu_memory_gb:.2f} GB")
    print("ğŸ“ ç­–ç•¥: æ™ºèƒ½è‡ªåŠ¨åˆ†é…åˆ°GPU")
    device_map = "auto"
    
    # åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨æœ€ç®€é…ç½®
    print("ğŸ”„ å¼€å§‹è°ƒç”¨ AutoModelForCausalLM.from_pretrained...")
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {CHAT_MODEL_PATH}")
    
    import time
    start_time = time.time()
    
    try:
        print("â³ æ­£åœ¨å®ä¾‹åŒ–æ¨¡å‹...")
        torch.cuda.empty_cache()
        print("ğŸ§¹ æ¸…ç†GPUç¼“å­˜")
        
        # é…ç½®é‡åŒ–å‚æ•° - ä½¿ç”¨æ–°çš„BitsAndBytesConfig
        from transformers import BitsAndBytesConfig
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        chat_model = AutoModelForCausalLM.from_pretrained(
            CHAT_MODEL_PATH,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            torch_dtype=torch.float16,  # ä½¿ç”¨float16è·å¾—æ›´å¥½æ€§èƒ½
            device_map=device_map,
            quantization_config=quantization_config,  # ä½¿ç”¨æ–°çš„é‡åŒ–é…ç½®
            max_memory={0: f"{int(gpu_memory_gb * 0.85)}GB"},  # ä½¿ç”¨æ›´å¤šæ˜¾å­˜
            offload_folder=None,
            offload_state_dict=False,
            # æ™ºèƒ½é€‰æ‹©æ³¨æ„åŠ›å®ç°
            **(_get_attention_config())
        )
        
        end_time = time.time()
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’")
        
        # ä¸éœ€è¦æ‰‹åŠ¨ç§»åŠ¨åˆ°GPUï¼Œaccelerateå·²ç»å¤„ç†äº†
        print("âœ… æ¨¡å‹å·²é€šè¿‡accelerateè‡ªåŠ¨æ˜ å°„åˆ°GPU")
        
        # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨
        _print_gpu_status()
        
        print("ğŸ”„ å¼€å§‹æ£€æŸ¥æ¨¡å‹çŠ¶æ€...")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        raise e
    
    
    print("ğŸ”„ æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¼€å§‹åå¤„ç†...")
    chat_model.eval()
    print("âœ… æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼")
    
    # å…ˆæ‰“å°åŸºæœ¬çš„æ¨¡å‹åŠ è½½çŠ¶æ€
    print("âœ… Chatæ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡")
    print("ğŸ“Š Chatæ¨¡å‹è®¾å¤‡åˆ†å¸ƒ:")
    try:
        if hasattr(chat_model, 'hf_device_map'):
            device_map_items = list(chat_model.hf_device_map.items())[:5]  # åªæ˜¾ç¤ºå‰5ä¸ªé¿å…è¾“å‡ºè¿‡å¤š
            for layer, device in device_map_items:
                if isinstance(layer, str) and len(layer) > 50:
                    layer = layer[:47] + "..."
                print(f"   {layer}: {device}")
            if len(chat_model.hf_device_map) > 5:
                print(f"   ... å’Œå…¶ä»– {len(chat_model.hf_device_map) - 5} ä¸ªå±‚")
        else:
            print("   æœªæ‰¾åˆ°è®¾å¤‡æ˜ å°„ä¿¡æ¯")
    except Exception as e:
        print(f"   è®¾å¤‡æ˜ å°„æ£€æŸ¥å¤±è´¥: {e}")
    
    print("âœ… Chatæ¨¡å‹åŸºç¡€åŠ è½½å®Œæˆ (NF4é‡åŒ–)")
    
    # é›†æˆ LangChain
    print("ğŸ”— æ­£åœ¨é›†æˆ LangChain...")
    try:
        # ä½¿ç”¨æ­£ç¡®çš„ HuggingFacePipeline åˆå§‹åŒ–æ–¹å¼
        from transformers import pipeline

        # åˆ›å»º transformers pipeline
        hf_pipeline = pipeline(
            "text-generation",
            model=chat_model,
            tokenizer=chat_tokenizer,
            # ä¸æŒ‡å®šdeviceå‚æ•°ï¼Œå› ä¸ºæ¨¡å‹å·²é€šè¿‡accelerateç®¡ç†è®¾å¤‡
            model_kwargs={
                "temperature": 0.3,
                "max_new_tokens": 512,
                "do_sample": True,
                "pad_token_id": chat_tokenizer.eos_token_id,
                "eos_token_id": chat_tokenizer.eos_token_id,
            }
        )
        
        # åˆ›å»º LangChain Pipeline
        langchain_llm = HuggingFacePipeline(pipeline=hf_pipeline)
        
        # åˆ›å»º Chat æ¨¡å‹
        langchain_chat_model = ChatHuggingFace(llm=langchain_llm, verbose=False)
        
        # åˆ›å»ºå¯¹è¯æ¨¡æ¿
        chat_prompt_template = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚è¯·ç®€æ´ã€å‡†ç¡®åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸è¦åŒ…å«ä»»ä½•å¤šä½™çš„å†…å®¹æˆ–è§£é‡Šã€‚"),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}")
        ])
        
        print("âœ… LangChain é›†æˆå®Œæˆ")
        
    except Exception as e:
        print(f"âš ï¸ LangChain é›†æˆå¤±è´¥: {e}")
        langchain_llm = None
        langchain_chat_model = None
        chat_prompt_template = None

def _maybe_warmup():
    if os.getenv("CHAT_WARMUP", "0") != "1":
        return
    if chat_model is None:
        return
    try:
        print("Warmup start ...")
        t = chat_tokenizer("Hello", return_tensors="pt")
        # æ‰¾åˆ° chat_model çš„ä¸€ä¸ªå·²åŠ è½½è®¾å¤‡
        try:
            device = next(p.device for p in chat_model.parameters() if p.device.type != "meta")
            t = {k: v.to(device) for k, v in t.items()}
        except StopIteration:
            # å¦‚æœæ‰€æœ‰å‚æ•°éƒ½åœ¨ meta è®¾å¤‡ï¼Œä½¿ç”¨é»˜è®¤è®¾å¤‡
            t = {k: v.to(DEFAULT_DEVICE) for k, v in t.items()}
        with torch.no_grad():
            chat_model.generate(**t, max_new_tokens=4)
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        print("Warmup done")
    except Exception as e:
        print(f"Warmup failed: {e}")

@app.on_event("startup")
async def load_models():
    global embedding_tokenizer, embedding_model, reranker_tokenizer, reranker_model, chat_tokenizer, chat_model
    
    print("ğŸš€ å¯åŠ¨BGEæ¨¡å‹æœåŠ¡ - å•è¯·æ±‚ä¼˜åŒ–æ¨¡å¼...")
    print("=" * 60)
    print(f"âœ… GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
    
    os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"
    
    app.state.chat_config = {
        "quant": "nf4",  # ä½¿ç”¨NF4 4bité‡åŒ–
        "resolved_dtype": "float16",  # ä½¿ç”¨float16è·å¾—æ›´å¥½æ€§èƒ½
        "max_new_tokens_default": int(os.getenv("MAX_NEW_TOKENS", "512"))
    }
    
    print("ğŸ¯ é…ç½®ä¿¡æ¯:")
    print(f"   - é‡åŒ–æ–¹å¼: {app.state.chat_config['quant']}")
    print(f"   - æ•°æ®ç±»å‹: {app.state.chat_config['resolved_dtype']}")
    print(f"   - é»˜è®¤æœ€å¤§Token: {app.state.chat_config['max_new_tokens_default']}")
    
    try:
        print("\nğŸ“¦ å¼€å§‹åŠ è½½æ¨¡å‹...")
        _load_embedding()
        print()
        _load_reranker() 
        print()
        _load_chat()
        print()
        _maybe_warmup()
        
        print("\nâœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼")
        allocated = torch.cuda.memory_allocated(0) / (1024**3)
        total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"ğŸ® GPUæ˜¾å­˜ä½¿ç”¨: {allocated:.1f}GB / {total:.1f}GB ({allocated/total*100:.1f}%)")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½é”™è¯¯: {e}")
        raise e  # ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œä¸è¦ç»§ç»­è¿è¡Œ

@app.get("/")
async def root():
    return {"message": "BGE Model Service is running"}

@app.get("/health")
async def health_check():
    # æ”¶é›†GPUä¿¡æ¯
    gpu_info = {}
    if torch.cuda.is_available():
        gpu_info = {
            "gpu_available": True,
            "gpu_count": torch.cuda.device_count(),
            "current_device": torch.cuda.current_device(),
            "gpu_name": torch.cuda.get_device_name(0),
            "gpu_memory_allocated_gb": round(torch.cuda.memory_allocated(0) / (1024**3), 2),
            "gpu_memory_reserved_gb": round(torch.cuda.memory_reserved(0) / (1024**3), 2),
            "gpu_memory_total_gb": round(torch.cuda.get_device_properties(0).total_memory / (1024**3), 2)
        }
    else:
        gpu_info = {"gpu_available": False}
    
    # æ£€æŸ¥æ¨¡å‹è®¾å¤‡ä½ç½®
    model_devices = {}
    if embedding_model is not None:
        model_devices["embedding_device"] = str(next(embedding_model.parameters()).device)
    if reranker_model is not None:
        model_devices["reranker_device"] = str(next(reranker_model.parameters()).device)
    if chat_model is not None:
        # æ£€æŸ¥chatæ¨¡å‹çš„è®¾å¤‡åˆ†å¸ƒ
        gpu_layers = 0
        cpu_layers = 0
        for param in chat_model.parameters():
            if param.device.type == 'cuda':
                gpu_layers += 1
            else:
                cpu_layers += 1
        
        model_devices["chat_gpu_params"] = gpu_layers
        model_devices["chat_cpu_params"] = cpu_layers
        model_devices["chat_gpu_percentage"] = round(gpu_layers / (gpu_layers + cpu_layers) * 100, 1) if (gpu_layers + cpu_layers) > 0 else 0
    
    return {
        "status": "healthy", 
        "embedding_model_loaded": embedding_model is not None,
        "reranker_model_loaded": reranker_model is not None,
        "chat_model_loaded": chat_model is not None,
        "chat_quant": "mxfp4",
        "chat_dtype": getattr(app.state, 'chat_config', {}).get('resolved_dtype'),
        "max_new_tokens_default": getattr(app.state, 'chat_config', {}).get('max_new_tokens_default'),
        "gpu_info": gpu_info,
        "model_devices": model_devices
    }

@app.get("/stats")
async def stats():
    """ç®€åŒ–çš„ç»Ÿè®¡ä¿¡æ¯"""
    gpu_info = {}
    if torch.cuda.is_available():
        gpu_info = {
            "allocated_gb": round(torch.cuda.memory_allocated(0) / (1024**3), 2),
            "reserved_gb": round(torch.cuda.memory_reserved(0) / (1024**3), 2),
            "total_gb": round(torch.cuda.get_device_properties(0).total_memory / (1024**3), 2)
        }
    
    return {
        "gpu_memory": gpu_info,
        "models_loaded": {
            "embedding": embedding_model is not None,
            "reranker": reranker_model is not None,
            "chat": chat_model is not None
        },
        "chat_config": getattr(app.state, 'chat_config', {}),
    }

@app.post("/embeddings", response_model=EmbeddingResponse)
async def get_embeddings(request: TextRequest):
    if embedding_model is None or embedding_tokenizer is None:
        raise HTTPException(status_code=500, detail="Embedding model not loaded")
    
    try:
        # ç›´æ¥å¤„ç† - ä¸“æ³¨å•è¯·æ±‚æ€§èƒ½
        encoded_input = embedding_tokenizer(
            request.texts,
            padding=True,
            truncation=True,
            max_length=request.max_length,
            return_tensors='pt'
        )
        
        # ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
        device = next(embedding_model.parameters()).device
        encoded_input = {k: v.to(device, non_blocking=True) for k, v in encoded_input.items()}
        
        # ä¼˜åŒ–æ¨ç†
        with torch.no_grad():
            if torch.cuda.is_available():
                with torch.amp.autocast('cuda', dtype=torch.float16):
                    model_output = embedding_model(**encoded_input)
            else:
                model_output = embedding_model(**encoded_input)
            embeddings = model_output.last_hidden_state[:, 0].cpu().numpy()
        
        return EmbeddingResponse(embeddings=embeddings.tolist())
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error processing request: {str(e)}")

@app.post("/rerank", response_model=RerankResponse)
async def rerank_documents(request: RerankRequest):
    if reranker_model is None or reranker_tokenizer is None:
        raise HTTPException(status_code=500, detail="Reranker model not loaded")
    
    try:
        device = next(reranker_model.parameters()).device
        scores = []
        
        # æ‰¹é‡å¤„ç†æ‰€æœ‰æ–‡æ¡£å¯¹ - å•è¯·æ±‚ä¼˜åŒ–
        all_pairs = [[request.query, doc] for doc in request.documents]
        
        # ä¸€æ¬¡æ€§ç¼–ç æ‰€æœ‰æŸ¥è¯¢-æ–‡æ¡£å¯¹
        encoded_input = reranker_tokenizer(
            all_pairs,
            padding=True,
            truncation=True,
            max_length=request.max_length,
            return_tensors='pt'
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        encoded_input = {k: v.to(device, non_blocking=True) for k, v in encoded_input.items()}
        
        # ä¼˜åŒ–æ¨ç†
        with torch.no_grad():
            if torch.cuda.is_available():
                with torch.amp.autocast('cuda', dtype=torch.float16):
                    outputs = reranker_model(**encoded_input)
            else:
                outputs = reranker_model(**encoded_input)
            
            # æ‰¹é‡è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
            cls_outputs = outputs.last_hidden_state[:, 0].cpu().numpy()
            scores = [float(np.mean(score)) for score in cls_outputs]
        
        return RerankResponse(scores=scores)
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error processing rerank request: {str(e)}")

def _get_model_device(model):
    """è·å–æ¨¡å‹çš„è®¾å¤‡"""
    device = None
    if torch.cuda.is_available():
        try:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰device_map
            if hasattr(model, 'hf_device_map'):
                # è·å–ç¬¬ä¸€ä¸ªå¯ç”¨è®¾å¤‡
                device = next(iter(model.hf_device_map.values()))
                if isinstance(device, str):
                    device = torch.device(device)
                elif isinstance(device, int):
                    device = torch.device(f"cuda:{device}")
            else:
                # å°è¯•ä»æ¨¡å‹å‚æ•°è·å–è®¾å¤‡
                device = next(p.device for p in model.parameters() if p.device.type != "meta")
        except (StopIteration, AttributeError):
            device = torch.device(DEFAULT_DEVICE)
    else:
        device = torch.device("cpu")
    return device

def _generate_with_model(model, tokenizer, input_ids, attention_mask, max_new_tokens, temperature, do_sample, device):
    """ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬"""
    with torch.no_grad():
        if torch.cuda.is_available() and device.type == "cuda":
            with torch.amp.autocast('cuda'):
                outputs = model.generate(
                    input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=do_sample,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    use_cache=True,
                    num_beams=1,
                    early_stopping=True
                )
        else:
            outputs = model.generate(
                input_ids,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=do_sample,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                use_cache=True,
                num_beams=1,
                repetition_penalty=1.0,  # ç¦ç”¨é‡å¤æƒ©ç½š
                length_penalty=1.0,  # ç¦ç”¨é•¿åº¦æƒ©ç½š
                early_stopping=True,
                # æé€Ÿä¼˜åŒ–å‚æ•°
                top_k=20 if do_sample else None,
                top_p=0.9 if do_sample else None,
                output_attentions=False,
                output_hidden_states=False
            )
    return outputs

def _stream_generate_with_model(model, tokenizer, input_ids, attention_mask, max_new_tokens, temperature, do_sample, device):
    """ä¼˜åŒ–çš„æµå¼ç”Ÿæˆæ–‡æœ¬ - ä½¿ç”¨transformerså†…ç½®çš„æµå¼ç”Ÿæˆ"""
    import json
    import time
    import uuid
    from transformers import TextIteratorStreamer
    import threading

    # åˆ›å»ºå“åº”ID
    response_id = f"chatcmpl-{uuid.uuid4().hex[:24]}"
    created_time = int(time.time())
    
    try:
        # ä½¿ç”¨TextIteratorStreamerè¿›è¡Œæµå¼ç”Ÿæˆ
        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        # æé€Ÿä¼˜åŒ–çš„ç”Ÿæˆå‚æ•°
        generation_kwargs = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "max_new_tokens": max_new_tokens,
            "temperature": temperature,
            "do_sample": do_sample,
            "pad_token_id": tokenizer.eos_token_id,
            "eos_token_id": tokenizer.eos_token_id,
            "use_cache": True,
            "streamer": streamer,
            # æé€Ÿæ€§èƒ½ä¼˜åŒ–å‚æ•°
            "num_beams": 1,  # ç¦ç”¨beam search
            "repetition_penalty": 1.0,  # ç¦ç”¨é‡å¤æƒ©ç½š
            "length_penalty": 1.0,  # ç¦ç”¨é•¿åº¦æƒ©ç½š
            "early_stopping": False,  # æµå¼ç”Ÿæˆæ—¶ç¦ç”¨æå‰åœæ­¢
            "top_k": 20 if do_sample else None,  # é™åˆ¶é‡‡æ ·èŒƒå›´åŠ é€Ÿ
            "top_p": 0.9 if do_sample else None,  # æ ¸é‡‡æ ·åŠ é€Ÿ
            "output_attentions": False,  # ç¦ç”¨æ³¨æ„åŠ›æƒé‡è¾“å‡º
            "output_hidden_states": False,  # ç¦ç”¨éšè—çŠ¶æ€è¾“å‡º
        }
        
        # åœ¨å•ç‹¬çº¿ç¨‹ä¸­å¯åŠ¨ç”Ÿæˆ - ç®€åŒ–ç‰ˆæœ¬ä¸“æ³¨æ€§èƒ½
        def generate():
            with torch.no_grad():
                if torch.cuda.is_available() and device.type == "cuda":
                    with torch.amp.autocast('cuda', dtype=torch.float16):  # æ˜ç¡®æŒ‡å®šfloat16
                        model.generate(**generation_kwargs)
                else:
                    model.generate(**generation_kwargs)
        
        generation_thread = threading.Thread(target=generate)
        generation_thread.start()
        
        # æµå¼è¾“å‡ºæ¯ä¸ªç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        for new_text in streamer:
            if new_text:  # ç¡®ä¿ä¸æ˜¯ç©ºå­—ç¬¦ä¸²
                chunk = {
                    "id": response_id,
                    "object": CHAT_COMPLETION_CHUNK,
                    "created": created_time,
                    "model": "qwen3-30b-a3b-instruct-2507",
                    "choices": [{
                        "index": 0,
                        "delta": {"content": new_text},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(chunk)}\n\n"
        
        # ç­‰å¾…ç”Ÿæˆçº¿ç¨‹å®Œæˆ
        generation_thread.join()
        
        # å‘é€ç»“æŸchunk
        final_chunk = {
            "id": response_id,
            "object": CHAT_COMPLETION_CHUNK,
            "created": created_time,
            "model": "qwen3-30b-a3b-instruct-2507",
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": "stop"
            }]
        }
        yield f"data: {json.dumps(final_chunk)}\n\n"
        
    except Exception as e:
        # å‘é€é”™è¯¯ä¿¡æ¯
        error_chunk = {
            "id": response_id,
            "object": CHAT_COMPLETION_CHUNK,
            "created": created_time,
            "model": "qwen3-30b-a3b-instruct-2507",
            "choices": [{
                "index": 0,
                "delta": {"content": f"[ERROR: {str(e)}]"},
                "finish_reason": "error"
            }]
        }
        yield f"data: {json.dumps(error_chunk)}\n\n"
    
    # å‘é€ç»“æŸæ ‡è®°
    yield "data: [DONE]\n\n"

def _chat_with_langchain(messages: List[dict]):
    """ä½¿ç”¨ LangChain è¿›è¡ŒèŠå¤©æ¨ç†"""
    if langchain_chat_model is None or chat_prompt_template is None:
        return None
    
    try:
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        chat_history = []
        current_input = ""
        
        for msg in messages:
            if msg["role"] == "user":
                current_input = msg["content"]  # æœ€åä¸€ä¸ªç”¨æˆ·è¾“å…¥
            elif msg["role"] == "assistant":
                if current_input:  # å¦‚æœæœ‰ä¹‹å‰çš„ç”¨æˆ·è¾“å…¥
                    chat_history.append(HumanMessage(content=current_input))
                    current_input = ""
                chat_history.append(AIMessage(content=msg["content"]))
        
        # æ„å»ºprompt
        formatted_prompt = chat_prompt_template.format_messages(
            chat_history=chat_history,
            input=current_input
        )
        
        # ä½¿ç”¨ LangChain ç”Ÿæˆå›å¤
        response = langchain_chat_model.invoke(formatted_prompt)
        
        # æå–å¹¶æ¸…ç†å›å¤å†…å®¹
        if hasattr(response, 'content'):
            content = response.content.strip()
        else:
            content = str(response).strip()
        
        # æ¸…ç†Qwen3æ¨¡æ¿æ ‡è®° - ä½¿ç”¨å¸¸é‡æå‡æ€§èƒ½
        content = content.replace(QWEN_TEMPLATE_CONSTANTS["IM_END"], "").strip()
        content = content.replace(QWEN_TEMPLATE_CONSTANTS["IM_START_ASSISTANT"], "").strip()
        content = content.replace(QWEN_TEMPLATE_CONSTANTS["IM_START_SYSTEM"], "").strip()
        content = content.replace(QWEN_TEMPLATE_CONSTANTS["IM_START_USER"], "").strip()
        
        # å¦‚æœåŒ…å«assistantæ ‡è®°ï¼Œæå–assistantåçš„å†…å®¹
        if QWEN_TEMPLATE_CONSTANTS["IM_START_ASSISTANT"] in content:
            parts = content.split(QWEN_TEMPLATE_CONSTANTS["IM_START_ASSISTANT"])
            if len(parts) > 1:
                content = parts[-1].replace(QWEN_TEMPLATE_CONSTANTS["IM_END"], '').strip()
        
        return content
            
    except Exception as e:
        print(f"âŒ LangChain æ¨ç†å¤±è´¥: {e}")
        return None

@app.post("/chat", response_model=ChatResponse)
async def chat_completion(request: ChatRequest):
    if chat_model is None or chat_tokenizer is None:
        raise HTTPException(status_code=500, detail="Chat model not loaded")
    
    print(f"ğŸš€ Chatæ¨ç†: {len(request.messages)} æ¡æ¶ˆæ¯")
    
    try:
        # ä½¿ç”¨ LangChain æ¨¡å¼æ¨ç†
        if langchain_chat_model is not None:
            print("ğŸ”— ä½¿ç”¨ LangChain æ¨¡å¼æ¨ç†")
            assistant_response = _chat_with_langchain(request.messages)
            
            if assistant_response:
                print("âœ… LangChain æ¨ç†å®Œæˆ")
                return ChatResponse(
                    message=assistant_response,
                    model=CHAT_MODEL_PATH.split("/")[-1] + " (LangChain)"
                )
        
        # ä½¿ç”¨åŸç”Ÿæ¨¡å¼æ¨ç†
        print("ğŸ”§ ä½¿ç”¨åŸç”Ÿæ¨¡å¼æ¨ç†")
        
        # æ„å»ºQwen3æ ¼å¼çš„å¯¹è¯
        conversation = ""
        for message in request.messages:
            role = message["role"]
            content = message["content"]
            
            if role == "system":
                conversation += f"{QWEN_TEMPLATE_CONSTANTS['IM_START_SYSTEM']}\n{content}{QWEN_TEMPLATE_CONSTANTS['IM_END']}\n"
            elif role == "user":
                conversation += f"{QWEN_TEMPLATE_CONSTANTS['IM_START_USER']}\n{content}{QWEN_TEMPLATE_CONSTANTS['IM_END']}\n"
            elif role == "assistant":
                conversation += f"{QWEN_TEMPLATE_CONSTANTS['IM_START_ASSISTANT']}\n{content}{QWEN_TEMPLATE_CONSTANTS['IM_END']}\n"
        
        # æ·»åŠ assistantå¼€å§‹æ ‡è®°
        conversation += f"{QWEN_TEMPLATE_CONSTANTS['IM_START_ASSISTANT']}\n"
        
        # ç¼–ç è¾“å…¥
        encoded = chat_tokenizer(
            conversation,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        )
        
        # è·å–è®¾å¤‡å¹¶ç§»åŠ¨æ•°æ®
        device = _get_model_device(chat_model)
        input_ids = encoded["input_ids"].to(device, non_blocking=True)
        attention_mask = encoded["attention_mask"].to(device, non_blocking=True)
        
        # ç”Ÿæˆå›å¤
        max_new_tokens = min(request.max_length, getattr(app.state, 'chat_config', {}).get('max_new_tokens_default', request.max_length))
            
        outputs = _generate_with_model(
            chat_model, chat_tokenizer, input_ids, attention_mask,
            max_new_tokens, request.temperature, request.do_sample, device
        )
        
        # è§£ç è¾“å‡º
        response = chat_tokenizer.decode(outputs[0], skip_special_tokens=True)
        assistant_response = response[len(conversation):]
        
        # æ¸…ç†Qwen3ç‰¹æ®Šæ ‡è®°
        assistant_response = assistant_response.replace(QWEN_TEMPLATE_CONSTANTS["IM_END"], "").strip()
        assistant_response = assistant_response.replace(QWEN_TEMPLATE_CONSTANTS["IM_START_ASSISTANT"], "").strip()
        
        print("âœ… Chatæ¨ç†å®Œæˆ")
        
        return ChatResponse(
            message=assistant_response.strip(),
            model=CHAT_MODEL_PATH.split("/")[-1]
        )
    
    except Exception as e:
        print(f"âŒ Chatæ¨ç†å¤±è´¥: {str(e)}")
        torch.cuda.empty_cache()
        print("ğŸ§¹ å·²æ¸…ç†GPUç¼“å­˜")
        raise HTTPException(status_code=500, detail=f"Error processing chat request: {str(e)}")

@app.post("/v1/chat/completions")
async def openai_chat_completion(request: OpenAIChatRequest):
    """OpenAIå…¼å®¹çš„èŠå¤©å®ŒæˆAPI - æ”¯æŒæµå¼å’Œéæµå¼"""
    if chat_model is None or chat_tokenizer is None:
        raise HTTPException(status_code=500, detail="Chat model not loaded")
    
    try:
        # è½¬æ¢OpenAIæ ¼å¼åˆ°å†…éƒ¨æ ¼å¼
        internal_messages = []
        for msg in request.messages:
            internal_messages.append({
                "role": msg.role,
                "content": msg.content
            })
        
        # æ„å»ºQwen3æ ¼å¼çš„å¯¹è¯
        conversation = ""
        for message in internal_messages:
            role = message["role"]
            content = message["content"]
            
            if role == "system":
                conversation += f"{QWEN_TEMPLATE_CONSTANTS['IM_START_SYSTEM']}\n{content}{QWEN_TEMPLATE_CONSTANTS['IM_END']}\n"
            elif role == "user":
                conversation += f"{QWEN_TEMPLATE_CONSTANTS['IM_START_USER']}\n{content}{QWEN_TEMPLATE_CONSTANTS['IM_END']}\n"
            elif role == "assistant":
                conversation += f"{QWEN_TEMPLATE_CONSTANTS['IM_START_ASSISTANT']}\n{content}{QWEN_TEMPLATE_CONSTANTS['IM_END']}\n"
        
        # æ·»åŠ assistantå¼€å§‹æ ‡è®°
        conversation += f"{QWEN_TEMPLATE_CONSTANTS['IM_START_ASSISTANT']}\n"
        
        # ç¼–ç è¾“å…¥
        encoded = chat_tokenizer(
            conversation,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        )
        
        # è·å–è®¾å¤‡å¹¶ç§»åŠ¨æ•°æ®
        device = _get_model_device(chat_model)
        input_ids = encoded["input_ids"].to(device, non_blocking=True)
        attention_mask = encoded["attention_mask"].to(device, non_blocking=True)
        
        # æ ¹æ®streamå‚æ•°é€‰æ‹©æµå¼æˆ–éæµå¼
        if request.stream:
            # æµå¼å“åº”
            def generate_stream():
                try:
                    for chunk in _stream_generate_with_model(
                        chat_model, chat_tokenizer, input_ids, attention_mask,
                        request.max_tokens, request.temperature, True, device
                    ):
                        yield chunk
                except Exception as e:
                    print(f"âŒ æµå¼ç”Ÿæˆé”™è¯¯: {e}")
                    yield f"data: {{'error': '{str(e)}'}}\n\n"
            
            return StreamingResponse(
                generate_stream(),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Access-Control-Allow-Origin": "*"
                }
            )
        else:
            # éæµå¼å“åº”
            outputs = _generate_with_model(
                chat_model, chat_tokenizer, input_ids, attention_mask,
                request.max_tokens, request.temperature, True, device
            )
            
            # è§£ç è¾“å‡º
            response = chat_tokenizer.decode(outputs[0], skip_special_tokens=True)
            assistant_response = response[len(conversation):]
            
            # æ¸…ç†Qwen3ç‰¹æ®Šæ ‡è®°
            assistant_response = assistant_response.replace("<|im_end|>", "").strip()
            assistant_response = assistant_response.replace("<|im_start|>assistant", "").strip()
            
            # è®¡ç®—tokenæ•°é‡
            input_tokens = len(chat_tokenizer.encode(conversation))
            output_tokens = len(chat_tokenizer.encode(assistant_response))
            
            # ç”Ÿæˆå“åº”IDå’Œæ—¶é—´æˆ³
            import time
            import uuid
            
            response_id = f"chatcmpl-{uuid.uuid4().hex[:24]}"
            created_time = int(time.time())
            
            # è¿”å›OpenAIæ ¼å¼å“åº”
            return OpenAIChatResponse(
                id=response_id,
                created=created_time,
                model=request.model,
                choices=[
                    OpenAIChoice(
                        message=OpenAIMessage(
                            role="assistant",
                            content=assistant_response
                        ),
                        finish_reason="stop",
                        index=0
                    )
                ],
                usage=OpenAIUsage(
                    prompt_tokens=input_tokens,
                    completion_tokens=output_tokens,
                    total_tokens=input_tokens + output_tokens
                )
            )
        
    except Exception as e:
        print(f"âŒ OpenAIå…¼å®¹APIè°ƒç”¨å¤±è´¥: {e}")
        torch.cuda.empty_cache()
        raise HTTPException(status_code=500, detail=f"Error processing OpenAI chat request: {str(e)}")

if __name__ == "__main__":
    print("ğŸŒŸ å¯åŠ¨BGEæ¨¡å‹æœåŠ¡å™¨")
    print("ğŸ® GPUæ£€æŸ¥:")
    _print_gpu_status()
    
    import uvicorn
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8000,
        log_level="info",
        access_log=True
    )